{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Upload & Inference Demo\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Upload your fine-tuned model weights to Hugging Face Hub.\n",
    "2. Download the weights from the Hub.\n",
    "3. Run inference using the downloaded weights.\n",
    "4. Save output images to a new folder in the `notebooks` directory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install huggingface_hub diffusers transformers safetensors torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from huggingface_hub import HfApi, HfFolder, upload_file, notebook_login\n",
    "import os\n",
    "\n",
    "# --- User Variables ---\n",
    "repo_id = 'TheoMefff/flux_schnell_baroque'  # Change if needed\n",
    "local_model_path = '../output/flux_schnell/flux_schnell_000002000.safetensors'  # Update if needed\n",
    "destination_name = os.path.basename(local_model_path)\n",
    "\n",
    "# Authenticate with your Hugging Face token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Upload the model weights to the Hugging Face Hub\n",
    "api = HfApi()\n",
    "token = HfFolder.get_token()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=local_model_path,\n",
    "    path_in_repo=destination_name,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    token=token\n",
    ")\n",
    "print(f'Uploaded {destination_name} to {repo_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download from Hugging Face and Run Inference\n",
    "We'll use the `diffusers` library to load the model and generate images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Download model snapshot from Hugging Face\n",
    "model_dir = snapshot_download(repo_id, repo_type=\"model\")\n",
    "print(f'Model downloaded to: {model_dir}')\n",
    "\n",
    "# Create output folder inside notebooks/\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\")) if '__file__' in globals() else os.getcwd()\n",
    "output_dir = os.path.join(notebook_dir, 'inference_outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Prompts for inference\n",
    "prompts = [\n",
    "    \"a beautiful painting of a sunset\",\n",
    "    \"a futuristic cityscape at night\",\n",
    "    \"a portrait of a woman in renaissance style\"\n",
    "]\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    image = pipe(prompt).images[0]\n",
    "    out_path = os.path.join(output_dir, f'output_{idx+1}.png')\n",
    "    image.save(out_path)\n",
    "    print(f'Saved: {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
